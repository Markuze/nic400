\section{Evaluation}
This section describes the in-depth analysis of 16 micro-benchmark experiments.
The experiments are partitioned into four sections; single core/multi-core and TCP/UDP.
On single core we evaluate TX/RX and RR on multi-core we evaluate TX/RX and Bi-Directional traffic.

\input{eval_exec_summery.tex}
\input{methodology.tex}

\subsection{Single-core UDP}
All single core tests are aimed to utilize 100\% of the CPU. In UDP-TX we have a single netperf flow sending 63KB datagrams. Using a dedicated memory allocator, boosts performance by 17.4\% in single core UDP TX experiment.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{figures/single_udp_tx_Bandwidth.pdf}
\caption{\label{fig:s-u-tx} Dan, I know that the text on the graph is too small. Single-core UDP TX.}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{figures/single_udp_tx_Ins_Packet.pdf}
\caption{\label{fig:s-u-tx-ipp} Single-core UDP TX, Instructions per packet.}
\end{figure}

In the case of single UDP TX, the main benefit is from faster allocation API. In figure \ref{fig:s-u-tx-ipp} we see the number of instructions per packet. The instructions per packet has the inverse ratio to the B/W, with - vs 3894 cycles to vanilla and \oursys respectively. In table \ref{tab:s-u-tx-funcs_child} we see the break down of cycles per packet. It seems that the major bulk of the cycles that are saved by \oursys per packet come from a shorter allocation path. Table \ref{tab:s-u-tx-funcs_child} breaks down the per cycle cost. The process of collecting perf measurements has a noticable side effect on the test, around 10\% degradation in performance. Due to this limitation we can only reliably count the packets, irqs and cycles during the test. Other perf status/pcm measurements are not reliable when looked at in conjunction with perf record, as these tests don't seem to have noticeable side effects.
The system is running with turbo mode, with \oursys running at 2.953GHZ and vanilla running at 3.002GHZ with nominal CPU at 2GHZ. This amounts to 1.66\% difference in clock speed. With perf \oursys has sent 742054 packets vs 633969 of vanilla, a 17\% improvement. During this test \oursys has experienced 57834 interrupts vs 56955 by vanilla, a 1.5\% difference. These numbers correspond to 12.8 and 10.96 packets per interrupt per tested system, both well below the NAPI budget. 
Conclusion, although the system with \oursys seems to show better l2 and llc utilization(not seen in the above discussion).
The instructions per packet seem to provide the correct answer. In addition the with an equal number of IRQs faster handling results in less IRQs per packet, resulting in less instructions per packet. Presumably there is also less work per IRQ but the overhead is not zero. Another way of putting it; batching is less effective.

\begin{table*}
\centering
\begin{tabular}{l|r|r|r}
Function & \oursys (3980)& vanilla (4736) & delta (19\% 756)\\\hline
csum\_partial\_copy\_generic* & 47.02\% (1871) & 35.24\% (1669)& -11\% -202\\
udp\_send\_skb & 22.51\% (896)& 22.36\% (1059) & +18.2\% +163\\
ret\_from\_intr & 11.09\% (441)& 20.03\% (949) & +115\% +508\\
alloc\_skb & 5.9\% (235)& 9.06\% (429)& +83\% +194\\
alloc\_skb\_with\_frags* & 0.12\% (4)& 0.09\% (4)& -\\
other & 13.36\%(532)& 13.22\% (626)& +17.7\% +94\\\hline
skb\_release\_all & 4.11\% (164) & 10.45\% (495)& +202\% +331\\\hline
\end{tabular}
\caption{\label{tab:s-u-tx-funcs_child}Function breakdown with child.(single core)}
\end{table*}

\subsubsection{IRQ separation - UDP TX}
The new allocation scheme has an impact both; on allocation that happens in the process context following a system call, and in IRQ context following a send completion event. We modify the single UDP experiment in such a way, that, the process is running on one core and all IRQs are handled on a second core on the same socket. This separation allows us to better gauge the effects of our code changes. Number are based on 36 runs.

\subsubsection{sock\_alloc\_send\_skb**}
This functions accounts for 40\% of the benifts we gain.
The benefit comes not only from cutting down the instruction count, but also
by faster freeing. In the unmodified kernel vanilla spends about 4\% waiting for space in the send buffer, which is "refilled" by the TX completions. These cycles account for 180 cycles per packet on average.
Removing this check from the function increases performance from 44Gb/s to 49Gb/s.

\subsubsection{Single core surprises}
Add table with difference between single and dual core experiments.
\begin{table}[]
    \centering
    \begin{tabular}{c|c|c|c}
       Param & \oursys & vanilla & diff\\\hline
        Freq GHZ & 3.137 & 3.134 & 0.09\%\\
        packets & 845,993 & 693,809 & 21.93\%\\
        irq & 56,224 & 58,201 & 3.51\%\\
        CPU & 100\% + 31.78\% & 100\% + 35.64\%& --\\
        IPC & 1.64 + 0.53 & 1.47 + 0.51 & --\\
        L2 misses & 15M + 17M & 51M + 33M & --\\
        cycles/packet & 3,708 & 4,517\\\hline
    \end{tabular}
    \caption{Run Parameters}
    \label{tab:run_parameters_utx_2}
\end{table}

\begin{table*}
\centering
\begin{tabular}{l|r|r|r}
Function & \oursys (3,708)& vanilla (4,517) & delta (21.82\%) 809\\\hline
csum\_and\_copy\_from\_iter\_full & 50.4\% (1868 +/-32) & 40.49\% (1829 +/-45)& -39\\
udp\_send\_skb & 24.08\% (893+/-35)& 20.25\% (915+/-21) &  +22\\
sock\_wmalloc & 12.04\% (446+/-11) & 19.04\%(860+/-13) & +414\\
sock\_alloc\_send\_skb** & 1.79\% (66+/-5)& 8.7\%(393+/-27) & +327\\
other & 12.05\% (447)& 11.52\%(520)& +73 \\\hline
\end{tabular}
\caption{\label{tab:2-u-tx-funcs_child_}Function breakdown with child.(single core)}
\end{table*}

\subsection{Single UDP RR}
The answer is a balance between better IPC and higher(!) instructions per packet. The higher instructions per packet is due do higher interrupt count. Faster processing leads to NAPI failing to keep up (need coraboration, with irq count and packet/irq).  