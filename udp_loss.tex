\subsection{UDP Losses}
Previously, we have seen that while, the udp sender is sending 100Gb/s of BW the receiver is seeing effectively 0 bytes in userspace. This phenomena was always observed, regardless of whether the netserver cpu was shared with the NAPI/irq cpu or not.

State I:  vanila 4.17: 
device driver reports 98Gb/s packets sent.
/proc/net/dev* reports high numbers of rx-drop packets.
netserver reports tens of Mb/s received. 

state I fix: Apparently  there is  a NET\_FLOW\_LIMIT**, which was dropping our packets.
There is a bit mask that controls whether the flow limit is active or not, but the only thing that helped was compiling the kernel w/o this option.

Result: The counter for rx-dropp was no longer rising. 

State II: vanila 4.17 w/o net\_flow\_limit.
device driver reports 98Gb/s packets sent.
netserver reports tens of Mb/s received.
dropwatch*** reports ip\_defrag as the main "hub" for dropped packets.

state II fix: There were several changes introduced between 4.16 and 4.17. Eric Dumazet, from Google was working DDOS prevention, where ip\_defrag was the target.
The fix was; increasing the number of buckets:
sudo sh -c "echo 16000000000 >/proc/sys/net/ipv4/ipfrag\_high\_thresh"
Result: 
Driver started reporting on ~50Gb/s received packets while netserver was seeing about 25Gb/s. Due to packets lost in transit, ip\_defrag was now dropping packets that couldn't be reassembled. And yes, pause parameters are configured both on the sender and the receiver. The driver was now slower than the network and packets would get lost. 

fix II: I've modified the test to send 8KB udp packets, more system calls but no need to defrag the packets.

Result: Driver started reporting 42Gb/s received, with netserver reporting 38Gb/s.
dropwatch was now pointing to udp\_enqueue. Increasing the receive socket size
brought the netserver report up to 42Gb/s.
The short version of the above is that three things needed to maintain sane UDP RX results. The first thing that must be fixed is the NET\_FLOW\_LIMIT issue, the second thing is the size of received packets and the third is the size of the UDP socket.
The open question is; weather, its possible in software to decrease the number of lost fragments in flight. With PFC on this shouldn't have been an issue.

With DAMN, nothing seems to help. The RX counetrs show 95Gb/s, but about 90\% of those are dropped at \newline udp\_queue\_rcv\_skb. /proc/net/snmp is showing InErrors on RevbufErrors, this is inherent to ENOMEM error in \newline \_\_udp\_queue\_rcv\_skb. The difference stems from the speed of freed/recycled memory. The difference is seen in the delta between rx\_packets and rx\_packets\_phy, in our experiments the discrepancy is found in rx\_out\_of\_buffer ethtool counter, namely those packets that were dropped by the driver due to a lack of memory.  
\subsection{UDP}
\begin{itemize}
    \item Performance degradation at 4.18
    \begin{itemize}
        \item UDP RX effective RX is 50Mb/s
        \item UDP TX is 30GB/s vs 45Gb/s in 4.14
    \end{itemize}
    \item Damn with fragments - decreasing the sock size increases performance
    
\end{itemize}
Next Questions:
\begin{itemize}
    \item 64KB RX on Damn, where are the packets lost?
    \item budget limiting.
    \item schedule napi on TX, no IRQ.
\end{itemize}
\subsection{proc utilities}
https://www.kernel.org/doc/Documentation/sysctl/net.txt.
Q: Would, experiments with netdev\_budget\_usecs balance userspace/napi better.
Lowering usec to 50 usec and budget to 4, has no effect.
\subsection{IRQ observations}
TX traffic, TCP and UDP, generates around 50K irqs per sec. This is due to the fact that only RX packets are counted for napi budget. TCP RX generates around 25K irqs per second, with 30 packets handled on average.  